"""
SimAMæ³¨æ„åŠ›æœºåˆ¶å®ç°
æ— å‚æ•°ã€è½»é‡çº§ã€é«˜æ•ˆçš„æ³¨æ„åŠ›æ¨¡å—
"""

import torch
import torch.nn as nn


class SimAM(nn.Module):
    """
    Simple, Parameter-Free Attention Module (SimAM)

    è®ºæ–‡: SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks
    ç‰¹ç‚¹:
    - æ— é¢å¤–å‚æ•°
    - è®¡ç®—é«˜æ•ˆ
    - é€‚åˆå°ç›®æ ‡æ£€æµ‹
    """

    def __init__(self, e_lambda=1e-4):
        """
        å‚æ•°:
            e_lambda: èƒ½é‡å‡½æ•°çš„æ­£åˆ™åŒ–å‚æ•°
        """
        super(SimAM, self).__init__()
        self.activation = nn.Sigmoid()
        self.e_lambda = e_lambda

    @staticmethod
    def __get_module_name():
        return "SimAM"

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å›¾ [B, C, H, W]

        è¿”å›:
            è¾“å‡ºç‰¹å¾å›¾ [B, C, H, W]
        """
        b, c, h, w = x.size()

        # è®¡ç®—æ¯ä¸ªé€šé“çš„ç©ºé—´ç»´åº¦ç»Ÿè®¡é‡
        # n: ç©ºé—´ç»´åº¦çš„å…ƒç´ æ•°é‡
        n = w * h - 1

        # è®¡ç®—å‡å€¼å’Œæ–¹å·®
        # x_minus_mu_square: (x - Î¼)^2
        x_minus_mu_square = (x - x.mean(dim=[2, 3], keepdim=True)).pow(2)

        # è®¡ç®—èƒ½é‡å‡½æ•° E
        # E_inv = 4 * (Ïƒ^2 + Î») / ((x - Î¼)^2 + 2Ïƒ^2 + 2Î»)
        y = x_minus_mu_square / (
                4 * (x_minus_mu_square.sum(dim=[2, 3], keepdim=True) / n + self.e_lambda)
        ) + 0.5

        # åº”ç”¨Sigmoidæ¿€æ´»
        return x * self.activation(y)


class SimAM_Optimized(nn.Module):
    """
    ä¼˜åŒ–ç‰ˆSimAM - æ·»åŠ å¯é€‰çš„é€šé“ç»´åº¦å¤„ç†
    """

    def __init__(self, channels=None, e_lambda=1e-4):
        super(SimAM_Optimized, self).__init__()
        self.activation = nn.Sigmoid()
        self.e_lambda = e_lambda

    def forward(self, x):
        b, c, h, w = x.size()
        n = w * h - 1

        # è®¡ç®—ç©ºé—´æ³¨æ„åŠ›ï¼ˆSimAMåŸå§‹æ–¹æ³•ï¼‰
        x_mean = x.mean(dim=[2, 3], keepdim=True)
        x_var = (x - x_mean).pow(2).sum(dim=[2, 3], keepdim=True) / n

        # è®¡ç®—èƒ½é‡
        x_minus_mu_square = (x - x_mean).pow(2)
        y = x_minus_mu_square / (4 * (x_var + self.e_lambda)) + 0.5

        # åº”ç”¨æ³¨æ„åŠ›
        return x * self.activation(y)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•SimAMæ¨¡å—
    print("=" * 70)
    print("ğŸ§ª æµ‹è¯•SimAMæ³¨æ„åŠ›æœºåˆ¶")
    print("=" * 70)

    # åˆ›å»ºæ¨¡å—
    simam = SimAM()

    # åˆ›å»ºæµ‹è¯•è¾“å…¥ [batch, channels, height, width]
    x = torch.randn(2, 64, 32, 32)

    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")

    # å‰å‘ä¼ æ’­
    y = simam(x)

    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in simam.parameters())} (åº”ä¸º0)")

    # æ£€æŸ¥è¾“å‡º
    assert x.shape == y.shape, "è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
    print("âœ… SimAMæµ‹è¯•é€šè¿‡!")

    print("=" * 70)